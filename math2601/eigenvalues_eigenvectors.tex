\section{Eigenvalues and Eigenvectors}

\subsection{Eigenvalues and Eigenvectors}
\paragraph{Definitions}
Suppose \(T \in L(V,V):\)
\begin{enumerate}
    \item if \(T(\vv) = \alpha\vv, \vv \in V, \alpha \in \bF, \vv \neq 0\) then \(\alpha\) is an eigenvalue of \(T, \vv\) is an eigenvector for \(T\) corresponding to \(\alpha\).
    \item If \(\lambda\) is an eigenvalue of \(T\), then the eigenspace of \(T, E_\lambda(T) = \{\vv \in V: T(\vv) = \lambda\vv \}\).
    \item We call the set of all eigenvalues of \(T\) the spectrum of \(T\).
\end{enumerate}

\paragraph{Notes}
\begin{enumerate}
    \item Eigenvectors are never zero.
    \item \(E_\lambda(T) = \{\vzero\} \cup \{\text{ all eigenvectors corresponding to } \lambda\}.\)
    \item Each eigenspace is invariant under T.
\end{enumerate}

\paragraph{Properties of Eigenvalues}
Let \(T: V \to V\) be linear. Then
\begin{enumerate}
    \item \(E_\lambda(T) = \ker(\lambda\id - T)\).
    \item If \(\lambda_1, \dots, \lambda_k\) are distinct eigenvalues and \(T(\vv_i) = \lambda_i\vv_i\), then \(\vv_1, \dots, \vv_k\) are linearly independent.
    \item If \(\lambda \neq \mu\) then \(E_\lambda(T) \cap E_\mu(T) = \{\vzero\}\).
\end{enumerate}

\paragraph{Diagonalisable}
A matrix \(A \in M_{p,p}(\bF)\) is diagonalisable over \(\bF\) if there is an invertible matrix \(P \in \GL(p,\bF)\) such that \(P^{-1}AP\) is diagonal - that is, A is similar over \(\bF\) to a diagonal matrix. A linear map \(T: V \to V\) is diagonalisable if there is a basis of \(V\) with respect to which the matrix of \(T\) is diagonal.

\paragraph{Diagonalisability Regarding Basis}
If \(T \in L(V,v)\) where \(V\) is a finite dimensional vector space over \(\bF\), then \(T\) is diagonalisable if and only if \(V\) has a basis whose elements are all eigenvectors of \(T\). Similarly, \(A \in M_{p,p}(\bF)\) is diagonalisable if and only if \(\bF^p\) has a basis consisting of eigenvectors of \(A\). 

\paragraph{Diagonalisability Regarding Eigenvalues}
A \(p \times p\) matrix with \(p\) distinct eigenvalues is diagonalisable.

\subsection{The Characteristic Polynomial}
\paragraph{Definition}
If \(A\) is a \(p \times p\) matrix over \(\bF\) then the characteristic polynomial of \(A\) is
\[\charp_A(t) = \det(tI - A).\]

\paragraph{Similarity Invariant}
The characteristic polynomial is a similarity invariant.

\paragraph{Characteristic Polynomials of Maps}
If \(V\) is finite dimensional and \(T \in L(V,V)\) the characteristic polynomial of \(T, \charp_T(t)\), is defined to be \(\charp_A(t)\) for any matrix \(A\) of \(T\).

\paragraph{Properties with Linear Maps}
Suppose \(T \in L(V,V)\), with \(\dim V = n\). Then
\begin{enumerate}
    \item \(\lambda\) is an eigenvalue if and only if \(\charp_T(\lambda) = 0\).
    \item \(W \leq E_\lambda(T)\) implies \(W\) is \(T\)-invariant.
    \item \(\lambda\) is an eigenvalue if and only if \(\nullity(T - \lambda\id) > 0\).
\end{enumerate}

\paragraph{Eigenvalues with Basis}
Let \(T: V \to V\) be linear on finite dimensional space \(V\), and \(\basis\) be a basis for \(V\). Let \(A\) be the matrix of \(T\) with respect to \(\basis\). The following are equivalent:
\begin{itemize}
    \item \(\vv\) is an eigenvector of \(T\) corresponding to eigenvalue \(\lambda\);
    \item the coordinate vector \([\vv]_\basis\) is an eigenvector of \(A\) corresponding to eigenvalue \(\lambda\).
\end{itemize}

\paragraph{Diagonalisation Algorithm}
Given matrix \(A\)
\begin{enumerate}
    \item Calculate \(\charp_A(t)\).
    \item Find all roots \(\alpha\) of \(\charp_A(t)\).
    \item For each \(\alpha\), calculate \(\dim(E_\alpha(A))\).
    \item If \(\sum_\alpha \dim(E_\alpha(A)) = n\) then \(A\) is diagonalizable.
    \item If \(A\) is diagonalisable construct \(P\), whose columns are a basis of \(V\) consisting of eigenvectors of \(A\) so that \(D = P^{-1}AP\) is diagonal.
\end{enumerate}

\subsection{Multiplicities}
\paragraph{Geometric and Algebraic Multiplicity}
Suppose \(T\) is a linear map on finite dimensional vector space \(V\). Suppose further that \(\lambda\) is an eigenvalue of \(T\) so that \(t - \lambda\) is a factor of the characteristic polynomial of \(T\). 
\begin{itemize}
    \item The geometric multiplicity (g.m.) of \(\lambda\) is \(\dim(E_\lambda(T))\).
    \item The algebraic multiplicity (a.m.) of \(\lambda\) is its multiplicity as a factor of \(\charp_T(t)\).
\end{itemize}
Let \(T: V \to V\) be linear on finite dimensional space \(V\) and \(\lambda\) an eigenvalue of \(T\). Then
\[1 \leq \text{geometric multiplicity } \lambda \leq \text{algebraic multiplicity } \lambda.\]

\paragraph{Solutions, Determinant and Trace with Eigenvalues}
Suppose \(A \in M_{p,p}(\bC)\). Then \(A\) has \(p\) eigenvalues \(\alpha_1, \dots, \alpha_p\) counting algebraic multiplicities. Also
\[\det(A) = \prod_{i=1}^p \alpha_i \text{ and } \tr(A) = \sum_{i=1}^p \alpha_i.\]

\paragraph{Equivalent Properties}
Let \(T: V \to V\) be linear on finite dimensional space \(V\). The following four statements are equivalent:
\begin{enumerate}
    \item \(T\) is diagonalisable.
    \item There is a basis for \(V\) consisting of eigenvectors for \(T\).
    \item \(V = E_{\lambda_1}(T) \oplus E_{\lambda_2}(T) \oplus \cdots \oplus E_{\lambda_k}(T)\) where \(\lambda_1, \dots, \lambda_k\) are the distinct eigenvalues of \(T\).
    \item The sum of the geometric multiplicities of the distinct eigenvalues is \(\dim V\). That is,
    \[\sum_{j=1}^k \dim E_{\lambda_j}(T) = \dim V.\]
\end{enumerate}

\paragraph{Diagonalizable Distinct Roots}
Linear map \(T\) on a \(p\)-dimensional space \(V\) over \(\bF\) is diagonalizable if \(\charp_T(t)\) has \(p\) distinct roots in \(\bF\).

\subsection{Normal Operators}
\paragraph{Normal Definition}
A linear transformation on an inner product space is normal if and only if \(T^* \circ T = T \circ T^*\) and an \(n \times n\) matrix is normal if and only if \(A^*A = AA^*\).

\paragraph{Inner Product Spaces and Normal Linear Maps}
Let \(V\) be a finite dimensional inner product space and \(T\) a normal linear map on \(V\). Then
\begin{enumerate}[label=(\alph*)]
    \item For all \(\vv \in V, ||T(\vv)|| = ||T^*(\vv)||\)
    \item For any scalar, \(\alpha, (T - \alpha\id)\) is also normal.
    \item If \(T\) has eigenvalue \(\lambda\) then \(\overline{\lambda}\) is an eigenvalue of \(T^*\).
    \item \(E_\lambda(T) = E_{\overline{\lambda}}(T^*)\).
    \item If \(\lambda \neq \mu\) then \(E_\lambda(T) \perp E_\mu(T)\).
\end{enumerate}

\paragraph{Algebraic and Geometric Multiplicity}
If \(T\) is normal with eigenvalue \(\lambda\) then the algebraic multiplicity of \(\lambda\) equals the geometric multiplicity of \(\lambda\).

\paragraph{The Spectral Theorem for Normal Operators}
If \(V\) is a finite dimensional inner product space over \(\bC\) and \(T \in L(V,V)\) is normal then there is an orthonormal basis \(\basis\) of \(V\) consisting of eigenvectors of \(T\). Hence if \(A \in M_{p,p}(\bC)\) is normal there is a unitary \(P\) such that \(P^{-1}AP = P^*AP\) is diagonal. Conversely if \([T]_\basis^\basis = A = PDP^{-1}\), with \(D\) diagonal and \(P\) unitary (so \(\basis\) is orthonormal) then both \(A\) and \(T\) are normal. 

\paragraph{Normal Operators and Matricies}
If \(A\) is normal with eigenvalues \(\lambda_i\), then there are matrices \(P_i\) with
\begin{align*}
    A = \sum_i \lambda_iP_i, & \qquad P_i^2 = P_i = P_i^*, \\
    P_iP_j = 0 \quad i \neq j, & \qquad \sum_i P_i = \id.
\end{align*}

\subsection{Self-Adjoint Maps and Matrices}
\paragraph{Self-Adjoint, Hermitian and Symmetric}
Suppose \(V\) is a finite dimensional inner product space over field \(\bF\) (where \(\bF\) is \(\bR\) or \(\bC\)) and \(T \in L(V,V))\).
\begin{enumerate}[label=\alph*)]
    \item If \(T\) is self-adjoint (that is Hermitian if \(\bF = \bC\) or symmetric if \(\bF = \bR\)) then the eigenvalues of \(T\) are real.
    \item If \(\bF = \bC\) and \(T\) is Hermitian there is a unitary basis for \(V\) consisting of eigenvectors of \(T\).
    \item If \(\bF = \bR\) and \(T\) is symmetric, then there is a real orthonormal basis of \(V\) consisting of eigenvectors of \(T\). 
\end{enumerate} 

\subsection{Unitary and Orthogonal Maps and Matrices}
\paragraph{Eigenvalues and Eigenvectors of a Unitary Map}
Suppose \(V\) is a \(p\)-dimensional inner product space over field \(\bF\) (where \(\bF\) is \(\bR\) or \(\bC\)), with \(T \in L(V,V)\) a unitary map. Then the eigenvalues of \(T\) lie on the unit circle in \(\bC\), that is are \(e^{i\alpha_k}\) for real \(\alpha_k\) and \(V\) has a unitary basis of eigenvectors of \(T\). 

\paragraph{Isometries}
Suppose \(T\) is an isometry on a real inner product space \(V\). Then its characteristic polynomial is of the form
\[\charp_T(t) = (t-1)^a(t+1)^b \prod_{j=1}^k(t - e^{i\alpha_j})(t-e^{-i\alpha_j})\]
where \(a + b + 2k = \dim(V)\) and \(\alpha_j \in (0 , \pi)\). Also there is an orthonormal basis of \(V\) in which the matrix of \(T\) is of the form
\[I_a \oplus (-I_b) \bigoplus_{j=1}^k R(\alpha_j).\]

\subsection{The Singular Value Decomposition}
\paragraph{Singular Value Decomposition (SVD)}
Let \(A \in M_{p,q}(\bC)\). A singular value decomposition of \(A\) is a factorization of \(A\) as \(U\sum V^*\) where \(U\) and \(V\) are square and unitary and \(\sum\) is a \(p \times q\) matrix with zero off diagonal terms and diagonal entries called the singular values satisfy
\[\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_q \geq 0.\]
The columns of \(U\) are called left singular vectors and the columns of \(V\) right singular vectors.

\paragraph{Kernel and Rank}
Let \(A \in M_{p,q}(\bF)\) for \(\bF\) either \(\bR\) or \(\bC\). Then
\begin{enumerate}[label = \alph*)]
    \item \(\ker(A*A) = \ker(A)\)
    \item \(\rank(A*A) = \rank(A)\)
\end{enumerate}

\paragraph{Existence of SVD}
For any matrix \(A\) an \(SVD\) exists and the singular values are unique.

\paragraph{Reduced Singular Value Decomposition}
If \(A \in M_{p,q}(\bC)\) is rank \(k\) then the reduced singular value decomposition is the decomposition
\[A = \hat{U}\hat{\Sigma}\hat{V}^*\]
where \(\hat{U}\) and \(\hat{V}\) have orthonormal columns and \(\hat{\Sigma}\) is \(k \times k\) invertible and diagonal.

\paragraph{Orthonormal Basis}
For any \(p \times q\) matrix of rank \(k\):
\begin{enumerate}[label = \alph*)]
    \item The last \(q -k\) right singular vectors are an orthonormal basis of \(\ker(A)\).
    \item The first \(k\) left singular vectors are an orthonormal basis of \(\im(A)\).
    \item \(\sum_{i=1}^k \omega_i^2 = \tr(\vec{A}^*\vec{A}) = \sum_{i=1}^p\sum_{j=1}^q |a_{ij}|^2.\)
\end{enumerate}

\paragraph{Pseudoinverse}
For an arbitrary \(A\) with reduced SVD \(\hat{U}\hat{\Sigma}\hat{V}^*\) we define the pseudoinverse of \(A, A^+\) by
\[A^+ = \hat{V}\hat{\Sigma}^{-1}\hat{U}^*.\]

\paragraph{Least Squares}
The least squares solution to \(A\vx = \vb\) for \(A\) of full rank is given by \(\vx = A^+\vb\).