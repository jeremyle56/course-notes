\section{Central Limit Theorem}

\subsection{Central Limit Theorem}
\paragraph{Central Limit Theorem}
Let \((X_n)_{n\in \bN_+}\) be an independent identically distributed sequence of random variables with common mean \(\mu = \bE(X_1)\) and variance \(\sigma^2 = \Var(X_1) < \infty\). Let \(\overline{X}_n = \frac{1}{n} \sum_{i=1}^n X_i\) then
\[\frac{\overline{X}_n - \mu}{\sigma / \sqrt{n}} \xrightarrow{d} Z \sim \mathcal{N}(0,1)\]

\subsection{Convergences}
\paragraph{Convergence in Distribution}
Let \((X_i)_{i \in \bN_+}\) be a sequence of random variables, we say that \(X_n\) converges to \(X\) in distribution if for all \(x\), for which \(F_X(x)\) is continuous
\[\lim_{n\to\infty} F_{X_n}(x) = F_X(x).\]
In this case, we write \(X_n \xrightarrow{d} X.\)

\paragraph{Convergence of Moment Generating Functions and Existence of CDF}
Let \((X_n)_{n\in \bN_+}\) be a sequence of r.v each with moment generating function \(M_{X_n}(t)\). Suppose that 
\[M(t) = \lim_{n \to \infty} M_{X_n}(t)\]
exists then there exists an unique valid cumulative distribution function \(F\) and r.v \(X\) such that \(F_X = F\).

\paragraph{Convergence of Random Variables}
A sequence of random variables \((X_n)_{n=1,\dots,}\) converges in probability to a r.v \(X\) if for all \(\epsilon > 0\),
\[\lim_{n \to \infty} \bP(|X_n - X| > \epsilon) = 0\]
and we write \(X_n \xrightarrow{\bP} X\).

\paragraph{Law of Large Numbers}
Let \((X_n)_{n \in \bN}\) be a sequence of independent r.vs with mean \(\mu\) and finite variance \(\sigma^2\), we set \(\overline{X}_n = \frac{1}{n}\sum_{i=1}^n X_i\), then
\[\overline{X}_n \xrightarrow{\bP}\mu\]

(Strong Version): Same Thing but using \textit{almost surely} probability for convergence.

\paragraph{Equal Almost Surely}
Two random variables \(X\) and \(Y\) are said to be equal almost surely if \(\bP(Y = X) = 1\) and we write \(X = Y\) a.s.

\paragraph{Almost Surely Convergence}
Given a random variable \(X\), a sequence \((X_n)_{n \in\bN}\) converges to almost surely to \(X\), if
\[\bP(\lim_{n\to 0} X_n = X) = 1\]
and we write \(X \xrightarrow{a.s} X\).

\paragraph{Convergence in \(L^p\)}
A sequence of random variables \((X_i)_{i \in \bN_+}\) is said to converge in \(L^p\) to another random variable \(X\) if for \(p \geq 1\),
\[\lim_{n \to \infty} \bE(|X_n - X|^p) = 0\]
in particular, if \(p = 2\), we say that \(X_n\) converges to \(X\) in the mean square sense.

\paragraph{Convergence in \(L^p\) and Probability}
Suppose \((X_n)_{n \in \bN}\) is a sequence of r.vs converging to \(X\) in \(L^p\) for \(p \geq 1\), then \(X_n\) converges to \(X\) in probability.
\[X_n \xrightarrow{L^p} X \implies X_n \xrightarrow{\bP} X\]

\paragraph{Convergence in Probability and Distribution}
Convergence in probability implies convergence in distribution. That is given \(X\) and a sequence \((X_n)_{n \in\bN_+}\),
\[X_n \xrightarrow{a.s.} X \implies X_n \xrightarrow{\bP} X \implies X_n \xrightarrow{d} X\]

\paragraph{Convergence Remark}
We have shown the following implications
\[X_n \xrightarrow{L^p} X \implies X_n \xrightarrow{\bP} X \implies X_n \xrightarrow{d} X\]

\subsection{Applications of the Central Limit Theorem}
\paragraph{Normal approximation to Binomial Distribution}
Suppose \(X \sim \Binomial(n,p)\) then
\[\frac{X - np}{\sqrt{np(1-p)}} \xrightarrow{d} \Normal(0,1)\]

\paragraph{Convergence to Constant in Distribution and Probability}
Suppose the sequence of r.vs \((X_n)_{n\in\bN}\) converges to a constant \(c\) in distribution, then \((X_n)_{n\in\bN}\) converges to a constant \(c\) in probability. That is
\[X_n \xrightarrow{d} c \implies X_n \xrightarrow{\bP}c\]

\paragraph{Continuous Mapping Lemma}
Suppose \(X_n \xrightarrow{\bP}X\) in probability then for any continuous function, \(g, g(X_n) \xrightarrow{\bP}g(X)\).

\paragraph{Slutsky' Theorem}
Let \((X_n)_{n\in\bN_+}\) be a sequence of r.vs converging to \(X\) in distribution and \((Y_i)_{i\in\bN_+}\) is another sequence of r.vs that converges in probability to a constnat \(c\), then
\begin{enumerate}
    \item \(X_n + Y_n \xrightarrow{d} X + c\)
    \item \(X_nY_n \xrightarrow{d} Xc\)
\end{enumerate}

\subsection{Delta Method}
\paragraph{Delta Method}
Let \(\frac{(X_n - \theta)}{\sigma - \sqrt{n}} \xrightarrow{d} Z \sim \Normal(0,1)\) and \(g\) is differentiable in a neighbourhood of \(\theta\) and \(g'(\theta) \neq 0\) then
\[\sqrt{n}(g(X_n) - g(\theta)) \xrightarrow{d} \Normal(0, \sigma^2[g'(\theta)]^2)\]

\paragraph{Extend Delta Method}
Let \(\frac{(X_n - \theta)}{\sigma - \sqrt{n}} \xrightarrow{d} Z \sim \Normal(0,1)\) and \(g\) is \(k\)-times differentiable in a neighbourhood of \(\theta\) and \(g^{(r)}(\theta) = 0\) for all \(r < k \in \bN\) then
\[n^{\frac{k}{2}}(g(Y_n) - g(\theta)) \xleftarrow{d} \frac{1}{k!}g^{(k)}(\theta)Z^k\]
As a special case, for \(k = 2\), we have that the limiting distribution is \(\mathcal{X}^2\).